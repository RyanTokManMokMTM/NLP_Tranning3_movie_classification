{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "web crawler",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP8OQYX/Lzo3zsrJwyM32So",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyanTokManMokMTM/NLP_Tranning3_movie_classification/blob/main/movie_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VViNTJ8cEyZH"
      },
      "source": [
        "## Classification the movie"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzXSIHHPUOea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7d012646-c124-48e9-aaa0-6ba19b409641"
      },
      "source": [
        "'''\n",
        "TODO:\n",
        "According Genre[0] for k,\n",
        "Acording to the\n",
        "'''"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nTODO:\\nAccording Genre[0] for k,\\nAcording to the\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbDFMxRNEw6_"
      },
      "source": [
        "#read the movie\n",
        "import csv\n",
        "import pandas as pd\n",
        "myDatas = []\n",
        "\n",
        "df = pd.read_csv(\"./trainMovies.csv\",encoding=\"utf-8\")\n",
        "header = df.columns.values.tolist()\n",
        "myDatas = df.values.tolist()\n",
        "\n",
        "\n",
        "#remove all extra characters in intro\n",
        "for i in myDatas:\n",
        "  i[3] = \"\".join(i[3].strip().split())"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiB7U6eudY6R"
      },
      "source": [
        "#remove all NA movie\n",
        "newDatas =  [movie for movie in myDatas if (type(movie[1]) == str)]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FX6S8qwe3ex9"
      },
      "source": [
        "x = [movie[0:1] + movie[2:] for movie in newDatas]\n",
        "y = [movie[1]  for movie in newDatas]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXF9P88lHyeq"
      },
      "source": [
        "# get all type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkoJRzF3r5Ok"
      },
      "source": [
        "movieType = set()\n",
        "for i in range(len(y)):\n",
        "  y[i] = y[i].split(\",\")[0]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG1kQSs5fOR-"
      },
      "source": [
        "# Feature\n",
        "\n",
        "\n",
        "1.   MovieName\n",
        "2.   ReleaseDate\n",
        "3.   Intro(Cut World)\n",
        "\n",
        "# Label\n",
        "1.   Genre[0]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwX9VTCo8dC5",
        "outputId": "9cb10b04-6731-425c-c34e-4adbf71aec19"
      },
      "source": [
        "!wget  https://raw.githubusercontent.com/foowaa/Chinese_from_dongxiexidian/master/stopwords.dat"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-15 12:48:13--  https://raw.githubusercontent.com/foowaa/Chinese_from_dongxiexidian/master/stopwords.dat\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13289 (13K) [text/plain]\n",
            "Saving to: ‘stopwords.dat.2’\n",
            "\n",
            "\rstopwords.dat.2       0%[                    ]       0  --.-KB/s               \rstopwords.dat.2     100%[===================>]  12.98K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-08-15 12:48:13 (93.2 MB/s) - ‘stopwords.dat.2’ saved [13289/13289]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_f0sJItETlI"
      },
      "source": [
        "#Load Stop word\n",
        "import codecs \n",
        "fp = codecs.open(\"./stopwords.dat\",\"r\",encoding=\"utf-8\")\n",
        "contents = fp.read()\n",
        "fp.close()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qf_d_JZb-Oy1",
        "outputId": "11fdbf4c-431c-4883-a9d0-eacf20b65970"
      },
      "source": [
        "#from sklearn import feature_extraction\n",
        "import jieba\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#get the word\n",
        "#clean all the stop word\n",
        "movDatas = x[:6000]\n",
        "movDatasLable = y[:6000]\n",
        "tempCutList = []\n",
        "for data in movDatas:\n",
        "  cutDataList = []\n",
        "  cutDataList += [word for word in jieba.cut(data[2],cut_all=False) if word not in contents]\n",
        "  tempCutList.append(\" \".join(cutDataList))\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /tmp/jieba.cache\n",
            "Loading model cost 0.882 seconds.\n",
            "Prefix dict has been built successfully.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHEb_4GgCFbl"
      },
      "source": [
        "vectorizer = CountVectorizer() # counting each word in list\n",
        "transform = TfidfTransformer() # calculate TFIED value for each word\n",
        "\n",
        "tfidf = transform.fit_transform(vectorizer.fit_transform(tempCutList)) #t.fit =>cal tdidf ,#v.fit =>change words to t.f\n",
        "words = vectorizer.get_feature_names() # get all word \n",
        "\n",
        "weight = tfidf.toarray()\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSQnhhrFNGsz"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "data = pd.DataFrame(data=weight,columns=words)\n",
        "X_train,X_test,y_train,y_test = train_test_split(data,movDatasLable,test_size=0.15,random_state=101)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9XvFxt7IeZC",
        "outputId": "2c5753ca-f613-48e9-d776-1eb140d2793b"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "i = 1\n",
        "while i <= 3000:\n",
        "  knn = KNeighborsClassifier(n_neighbors=i)\n",
        "  knn.fit(X_train,y_train)\n",
        "  pred = knn.predict(X_test)\n",
        "  print(\"k:\",i,\",value:\",knn.score(X_test, y_test))\n",
        "  i += 2\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "k: 1 ,value: 0.7663125948406677\n",
            "k: 3 ,value: 0.6494688922610015\n",
            "k: 5 ,value: 0.5918057663125948\n",
            "k: 7 ,value: 0.543247344461305\n",
            "k: 9 ,value: 0.5796661608497724\n",
            "k: 11 ,value: 0.5796661608497724\n",
            "k: 13 ,value: 0.5735963581183612\n",
            "k: 15 ,value: 0.5705614567526556\n",
            "k: 17 ,value: 0.5887708649468892\n",
            "k: 19 ,value: 0.5644916540212443\n",
            "k: 21 ,value: 0.5690440060698028\n",
            "k: 23 ,value: 0.5614567526555387\n",
            "k: 25 ,value: 0.5508345978755691\n",
            "k: 27 ,value: 0.5614567526555387\n",
            "k: 29 ,value: 0.5644916540212443\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}